{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f161eb",
   "metadata": {},
   "source": [
    "# C964 Capstone Project - Fake News Classifier\n",
    "This project utilizes binary logistic regression to classify news articles between fake or true news.\n",
    "\n",
    "Kaggle Dataset: https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset\n",
    "\n",
    "This project implements multiple libraries to achieve the goal of classifying news articles:\n",
    " 1. **Pandas** - *for csv document parsing of datasets.*\n",
    " 2. **re & nltk** - *cleaning and formatting text to reduce noise for the model.*\n",
    " 3. **sklearn** - *Commonly known as scikit-learn, is a free and open-scource machine learning library. Provides all the tools for implementing, training, and testing a model.*\n",
    " 4. **seaborn & matplotlib** - *Work in tangent to display visual graphs of the training data.*\n",
    "\n",
    " Libraries included for ease of testing for course evaluators:\n",
    "  1. **requests** - *A library used for sending HTTP/1.1 requests to a url or ip. A default get request is sent for testing purposes.*\n",
    "  2. **BeautifulSoup & lxml parser** - *Used to parse the html returned from the requests get request.*\n",
    "\n",
    "## Dataset Parsing, Concatenation, and Split\n",
    "The pandas library is imported and used to:\n",
    "- Parse and Concatenate the fake and true datasets\n",
    "\n",
    "Additionally, a column to label true or false data is given with the name of 'label' and true or false as '1' or '0'.\n",
    "The dataset is shuffled and the ID for each row is reset to ensure no duplicates in ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f616955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fake_ds = pd.read_csv('dataset/Fake.csv')\n",
    "true_ds = pd.read_csv('dataset/True.csv')\n",
    "\n",
    "fake_ds['label'] = 0\n",
    "true_ds['label'] = 1\n",
    "\n",
    "ds = pd.concat([fake_ds, true_ds])\n",
    "ds = ds.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b47b4a1",
   "metadata": {},
   "source": [
    "## Dataset Cleaning\n",
    "The nltk library is utilized to download stopwords that are more or less words that do not add to the meaning or intent of a sentence.\n",
    "The re library is utilized to strip sentences of everythink that is not a number or letter.\n",
    "\n",
    "The article header and the article text are combined to replace the article text column, with the aforementioned stripped from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1e39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def process_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "ds['text'] = ds['title']  + ' ' + ds['text']\n",
    "ds['text'] = ds['text'].apply(process_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c96d933",
   "metadata": {},
   "source": [
    "## Creation of Test Dataset\n",
    "The test dataset is created, as 25% of the total dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13a33b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = ds['text']\n",
    "y = ds['label']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dfacb5",
   "metadata": {},
   "source": [
    "## Test and Train Sets Vectorization\n",
    "The test and train sets are vectorized into a format more understandable by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a1ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7, max_features=5000)\n",
    "x_train_vec = vectorizer.fit_transform(x_train)\n",
    "x_test_vec = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e196bfa5",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "The model is initialized and trained using the vectorized training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac132f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(x_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a8825f",
   "metadata": {},
   "source": [
    "## ROC Curve\n",
    "Creation and display of ROC (Receiver Operating Characteristic) Curve. Creates a visualization of the true to the false rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cc2f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "y_prob = model.predict_log_proba(x_test_vec)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "\n",
    "plot.plot(fpr, tpr, label='ROC Curve')\n",
    "plot.plot([0, 1], [0, 1], 'k--')\n",
    "plot.xlabel('False Positive Rate')\n",
    "plot.ylabel('True Positive Rate')\n",
    "plot.title('ROC Curve')\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b59079",
   "metadata": {},
   "source": [
    "## Accuracy Bar Graph\n",
    "Print, creation, and dispay of accuracy bar graph. This shows the training accuracy and percentage compared to the testing accuracy percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6adea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_acc = accuracy_score(y_train, model.predict(x_train_vec))\n",
    "test_acc = accuracy_score(y_test, model.predict(x_test_vec))\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc*100:.2f}\")\n",
    "print(f\"Testing Accuracy: {test_acc*100:.2f}\")\n",
    "\n",
    "plot.bar(['Training Accuracy', 'Testing Accuracy'], [train_acc, test_acc], color=['skyblue', 'salmon'])\n",
    "plot.ylim(0, 1)\n",
    "plot.title(\"Model Accuracy Comparison\")\n",
    "plot.ylabel(\"Accuracy\")\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e8ac83",
   "metadata": {},
   "source": [
    "## Confusion Map Creation and Display\n",
    "seaborn plots training data compared to actual data on a confusion matrix through matplotlib.\n",
    "The matrix is then shown to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec30a595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = model.predict(x_test_vec)\n",
    "\n",
    "sb.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'True'], yticklabels=['Fake', 'True'])\n",
    "\n",
    "plot.xlabel('Predicted')\n",
    "plot.ylabel('Actual')\n",
    "plot.title('confusion Matrix')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ff4538",
   "metadata": {},
   "source": [
    "## Function for Model Predictions\n",
    "A function to pass an url and the article to classify and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a72dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyArticle(url,article):\n",
    "    clean_input = process_text(article)\n",
    "    input_vector = vectorizer.transform([clean_input])\n",
    "    prediction = model.predict(input_vector)\n",
    "    print(f\"The article from: {url} is found to be {'Fake' if prediction[0]==0 else 'Real'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf9d4e6",
   "metadata": {},
   "source": [
    "## Webscrape a Supplied Article URL\n",
    "## and BeautifulSoup HTML Parsing w/ lxml\n",
    "The use of the requests library to scrape a given URL as long as it is a valid (supported) domain.\n",
    "The request is sent in a way to prevent blocking from the given domain, user-agent.\n",
    "The request is stored as variable r.\n",
    "\n",
    "Beatiful soup utilizes lxml to parse the result of request r from the previous code snippet.\n",
    "A headlineClass and bodyClass is applied based on the structure of the supplied, supported, domain.\n",
    "A call to the previous classifyArticle function, two code snippets ago, is made to classify the given article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f35dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parseArticle(article):\n",
    "    match article[\"articleDomain\"]:\n",
    "        case \"cnn.com\":\n",
    "            headlineClass = \"headline__text\"\n",
    "            bodyClass = \"article__content\"\n",
    "        case \"foxnews.com\":\n",
    "            headlineClass = \"headline\"\n",
    "            bodyClass = \"article-body\"\n",
    "\n",
    "    soup = BeautifulSoup(article[\"html\"].text, \"lxml\")\n",
    "    headline = soup.find(class_=headlineClass).get_text()\n",
    "    body = soup.find(class_=bodyClass).get_text()\n",
    "\n",
    "    classifyArticle(article[\"URL\"], headline + \" \" + body)\n",
    "\n",
    "article = None\n",
    "validDomains = { \"cnn.com\", \"foxnews.com\" }\n",
    "endCommands = { \"exit\", \"end\", \"quit\" }\n",
    "\n",
    "while True:\n",
    "    print(\"To terminate this program use the command: exit, end, or quit.\")\n",
    "    userInput = str(input(\"Paste an article's URL and hit enter: \"))\n",
    "\n",
    "    if userInput.lower() in endCommands:\n",
    "        break\n",
    "    \n",
    "    domain = next((_domain for _domain in validDomains if _domain in userInput), None)\n",
    "    if domain:\n",
    "            article = { \"articleDomain\": domain, \"URL\":userInput }\n",
    "    \n",
    "    if article is None or domain is None:\n",
    "        print(\"Invalid domain, please use news from cnn.com or foxnews.com\")\n",
    "        continue\n",
    "\n",
    "    headers = { \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:140.0) Gecko/20100101 Firefox/140.0\"}\n",
    "\n",
    "    article[\"html\"] = requests.get(article[\"URL\"], headers)\n",
    "    parseArticle(article)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
